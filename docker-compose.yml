# Deploy de API Para Aplicação de IA com Ollama, Streamlit e Docker Compose

name: container-lab

services:
  
  ollama:
    image: ollama/ollama:latest
    container_name: backend_ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    entrypoint: ["sh", "-c"]
    command: |
      "ollama serve &
      sleep 10 &&
      ollama pull llama3.2:1b &&
      wait"

  api:
    build: ./api
    container_name: api
    ports:
      - "8000:8000"
    depends_on:
      - ollama
    environment:
      - OLLAMA_HOST=ollama
      - OLLAMA_PORT=11434

  streamlit:
    build: ./streamlit_app
    container_name: frontend_streamlit
    ports:
      - "8502:8501"
    depends_on:
      - api
    environment:
      - API_URL=http://api:8000

volumes:
  ollama_data:
