# ğŸ³ğŸ’¬ Chatbot Ollama com Docker 
Este projeto implementa um chatbot utilizando o modelo Ollama, com arquitetura modular baseada em containers Docker. 
A aplicaÃ§Ã£o Ã© composta por trÃªs camadas principais: Backend, API e Frontend.


## âš™ï¸Tecnologias Utilizadas
- **Backend**
  - FastAPI â€” Framework moderno para APIs em Python
  - Pydantic â€” ValidaÃ§Ã£o de dados e tipagem
  - Ollama â€” ExecuÃ§Ã£o local de modelos LLM
- **Frontend**
  - Streamlit â€” Interface web interativa para visualizaÃ§Ã£o e interaÃ§Ã£o com a API
- **Infraestrutura**
  - Docker
  - Docker Compose

## ğŸš€ Como Executar

- Leia o arquivo LEIAME.txt

  1.Clone o repositÃ³rio:

            git clone https://github.com/jaquelinesfernandes/Chatbot_Ollama_Docker
            cd Chatbot_Ollama_Docker

  2.Execute com Docker Compose:
  
            docker-compose up --build

  3.Acesse os serviÃ§os:
      - Frontend (Streamlit): http://localhost:8501
      - Backend (FastAPI docs): http://localhost:8000/docs
  
## ğŸ§  Ollama
O serviÃ§o Ollama Ã© executado em um container separado e expÃµe uma API local para interaÃ§Ã£o com modelos como LLaMA, Mistral, entre outros. O backend FastAPI se comunica com o Ollama via HTTP.

## ğŸ“‚ OrganizaÃ§Ã£o dos Containers
| ServiÃ§o  | Porta | DescriÃ§Ã£o                      |
|----------|-------|--------------------------------|
| Backend  | 8000  | API REST com FastAPI           |
| Frontend | 8501  | Interface Streamlit            |
| Ollama   | 11434 | Servidor de modelos LLLM local |

## ğŸ“Œ ObservaÃ§Ãµes
- Certifique-se de que sua mÃ¡quina possui suporte para execuÃ§Ã£o de containers com acesso Ã  GPU (opcional para modelos maiores).
- O Ollama precisa ser configurado com o modelo desejado no Dockerfile ou via comandos no container.




